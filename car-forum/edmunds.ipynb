{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll be using the cars and conversations forum\n",
    "url = 'https://forums.edmunds.com/discussion/18576/general/x/edmunds-members-cars-conversations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of forum pages\n",
    "num_pages = 2572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the comments in a string\n",
    "all_comments = \"\"\n",
    "\n",
    "# go through each page\n",
    "for page_number in range(1, num_pages + 1):\n",
    "    # first page string of url isn't anything special\n",
    "    if(page_number == 1):\n",
    "        new_url = url\n",
    "    else:\n",
    "        # end url format is /p220 etc.\n",
    "        new_url = url + \"/p\" + str(page_number)\n",
    "    \n",
    "    # collect the url with requests library\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # get the html of the page in string form\n",
    "    page_html = page.text\n",
    "    \n",
    "    # create the BeautifulSoup object that takes in the html in str form and a html/xml parser of choice either html.parser or lxml\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    \n",
    "    # inspect the elements from the webpage to know where stuff is\n",
    "    # in this case the comments are under a <p> tag which are all under a <div class = \"Message userContent\"</div/ tag \n",
    "    # which are all under a <div class = \"MessageList DataList Comments\"> tag\n",
    "    \n",
    "    # Pull all text from the 'MessageList DataList Comments' divs\n",
    "    messagelist = soup.find(class_ = \"MessageList DataList Comments\")\n",
    "    \n",
    "    # within the 'MessageList DataList Comments' divs pull all text from 'Message userContent' divs\n",
    "    usermessages = messagelist.find_all(class_ =  \"Message userContent\")\n",
    "    \n",
    "    # now extract just the <p> tags from all comments! \n",
    "    for i in range(len(usermessages)):\n",
    "        # a user comment might have multiple <p> tags\n",
    "        user_comments = usermessages[i].find_all('p')\n",
    "        for j in range(len(user_comments)):\n",
    "            # remove <a>, <img>, <br> tags embedded in <p>\n",
    "            [s.extract() for s in user_comments[j]('a')]\n",
    "            [s.extract() for s in user_comments[j]('img')]\n",
    "            [s.extract() for s in user_comments[j]('br')]\n",
    "            \n",
    "            # add the users comments to the mega string\n",
    "            all_comments += user_comments[j].prettify()\n",
    "    \n",
    "    # print some checks\n",
    "    print('Page 'str(page_number) + 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some string manipulation\n",
    "# remove the newline characters, '<p>', '</p','said:'\n",
    "parsedData = all_comments.replace('\\n', '')\n",
    "parsedData = parsedData.replace(r\"\\'\", r\"'\")\n",
    "parsedData = parsedData.replace(r\"<p>\", \"\")\n",
    "parsedData = parsedData.replace(r\"</p>\", \"\")\n",
    "parsedData = parsedData.replace(r\"</p>\", \"\")\n",
    "parsedData = parsedData.replace(r\"said:\", \"\")\n",
    "parsedData = parsedData.replace(r\":\", \"\")\n",
    "parsedData = parsedData.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab word frequency using nltk library\n",
    "import nltk\n",
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download()\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text \n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a word count\n",
    "from collections import Counter\n",
    "\n",
    "# counter object\n",
    "words = Counter()\n",
    "\n",
    "# update counter with new words\n",
    "words.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "words.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these are the most common words, but they are also the same in any english text. Therefore they are not very insightful. Hence, let's remove single characters, numbers, and common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do this again: remove stop words, single letter variable names, numbers and et, al, cid\n",
    "words = Counter(x for x in tokens if x not in stopwords and x.isdigit() == False and len(x) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "words.most_common()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
